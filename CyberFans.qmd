---
title: "CyberFans"
format: html
editor: visual
---
#Установка библиотек
```{r}
install.packages(c("dplyr", "tidyr", "readr", "jsonlite", "arrow", "ggplot2", "lubridate", "rmarkdown"))
```



#Загрузка всех .csv файлов с гитхаба https://github.com/d-yacenko/dataset. Создание общего файла + очистка данных
```{r}
# Подключаем необходимые библиотеки
library(httr)
library(readr)
library(stringr)
library(dplyr)

# Функция для преобразования ссылок из /blob/ в /raw/
convert_to_raw_url <- function(blob_url) {
  gsub("/blob/", "/raw/", blob_url)
}

# Функция безопасной загрузки .csv файла с отладкой
load_csv_safe <- function(url) {
  tryCatch({
    message(paste("Попытка загрузки файла:", url))
    # Попытка загрузить файл
    data <- read_csv(url, show_col_types = FALSE)  # Отключаем вывод типов колонок
    message(paste("Файл успешно загружен:", url))
    return(data)
  }, error = function(e) {
    #произошла ошибка
    message(paste("Ошибка при загрузке файла:", url))
    message(paste("Описание ошибки:", e$message))
    return(NULL)  # Возвращаем NULL в случае ошибки
  })
}

# Проверяем доступность ссылок на .csv файлы
check_links <- function(links) {
  if (length(links) == 0) {
    stop("Не найдено ни одной ссылки на .csv файлы.")
  } else {
    message("Список ссылок на .csv файлы:")
    print(links)
  }
}

# Основная функция для загрузки файлов из GitHub и объединения их в одну таблицу
load_and_combine_csv_files <- function(repo_url, folder_path) {
  # Формируем URL для API GitHub
  api_url <- paste0("https://api.github.com/repos/",
                    str_replace(repo_url, "https://github.com/", ""),
                    "/contents/",
                    folder_path)
  
  # Отправляем запрос к API GitHub
  response <- GET(api_url)
  
  if (status_code(response) != 200) {
    stop("Ошибка при обращении к API GitHub. Проверьте URL репозитория и папки.")
  }
  
  # Получаем содержимое папки
  content <- content(response, as = "parsed")
  
  # Извлекаем ссылки на файлы .csv
  raw_csv_links <- sapply(content, function(file_info) {
    if (!is.null(file_info$download_url) && grepl("\\.csv$", file_info$name)) {
      return(file_info$download_url)
    } else {
      return(NULL)
    }
  })
  
  # Удаляем NULL значения
  raw_csv_links <- raw_csv_links[!sapply(raw_csv_links, is.null)]
  
  # Проверяем ссылки
  check_links(raw_csv_links)
  
  # Загружаем все .csv файлы с задержкой между запросами
  all_data <- lapply(raw_csv_links, function(link) {
    Sys.sleep(1)  # Задержка в 1 секунду между запросами
    load_csv_safe(link)
  })
  
  # Удаляем NULL значения из списка данных
  all_data <- all_data[!sapply(all_data, is.null)]
  
  if (length(all_data) == 0) {
    stop("Не удалось загрузить ни одного файла.")
  }
  
  # Объединяем все таблицы в одну
  combined_data <- bind_rows(all_data)
  
  message("Все файлы успешно объединены в одну таблицу.")
  
  return(combined_data)
}

# Тестируем функцию с репозиторием d-yacenko/dataset и папкой telecom10k/
repo_url <- "https://github.com/d-yacenko/dataset"
folder_path <- "telecom10k/"  # Путь к папке в репозитории

combined_data <- load_and_combine_csv_files(repo_url, folder_path)

# Выводим первые строки объединенной таблицы для проверки
print(head(combined_data))

```
#Запись файла 
```{r}
write_csv(combined_data, "combined_dataset.csv")
```
#Разделение датасетов по числам с 31.12 по 07.01
```{r}
#библиотеки
library(dplyr)
library(readr)

#Загрка датасета
combined_dataset <- read_csv("combined_dataset.csv")

# Проверка
str(combined_dataset)  # Убедимся, что столбец StartSession существует

# Преобразование StartSession в формат даты и времени
combined_dataset <- combined_dataset %>%
  mutate(
    StartSession = as.POSIXct(StartSession, format = "%d-%m-%Y %H:%M:%S"), # Указываем формат d m Y H M S
    date = as.Date(StartSession)  # Извлекаем только дату (без времени)
  ) %>%
  filter(!is.na(date))  # Удаляем строки с NA (если есть некорректные даты)

# Проверка доступности дат
available_dates <- unique(combined_dataset$date)
print(available_dates)  # Выводим доступные даты

# Указываем диапазон дат для фильтрации
start_date <- as.Date("2023-12-31")  # Начальная дата
end_date <- as.Date("2024-01-07")    # Конечная дата

# Проверка, есть ли данные в указанном диапазоне
if (any(available_dates >= start_date & available_dates <= end_date, na.rm = TRUE)) {
  # Фильтруем данные по диапазону дат
  filtered_data <- combined_dataset %>%
    filter(date >= start_date & date <= end_date)

  # Разделение данных на отдельные файлы по дням
  for (i in 0:7) {
    specific_date <- start_date + i
    specific_data <- filtered_data %>%
      filter(date == specific_date)

    if (nrow(specific_data) > 0) {
      output_file <- paste0("dataset_", specific_date, ".csv")
      write_csv(specific_data, output_file)
    }
  }
  
  message("Датасеты успешно разделены и сохранены.")
} else {
  stop("В указанном диапазоне дат нет данных. Доступные даты: ", paste(available_dates, collapse = ", "))
}

```

#Скачивание .parquet файлов
```{r}
library(httr)
library(rvest)
library(arrow)

# Функция для скачивания файлов
download_file <- function(url, save_path) {
  response <- GET(url, write_disk(save_path, overwrite = TRUE))
  if (response$status_code == 200) {
    message(paste("Скачан файл:", save_path))
  } else {
    message(paste("Не удалось скачать файл:", url, "(HTTP", response$status_code, ")"))
  }
}


download_parquet_files_from_github <- function() {
  # URL репозитория GitHub
  base_url <- "https://github.com/d-yacenko/dataset/tree/main/telecom10k"
  raw_base_url <- "https://raw.githubusercontent.com/d-yacenko/dataset/main/telecom10k/"
  
  # Папка для сохранения файлов
  save_directory <- "parquet_files"
  if (!dir.exists(save_directory)) dir.create(save_directory)
  
  # Скачиваем HTML-страницу с файлами
  response <- GET(base_url)
  if (response$status_code != 200) {
    stop(paste("Не удалось получить доступ к странице", base_url, "(HTTP", response$status_code, ")"))
  }
  
  # Парсим HTML страницу
  html_content <- content(response, as = "text", encoding = "UTF-8")
  parsed_html <- read_html(html_content)
  
  # Ищем ссылки на файлы .parquet
  links <- parsed_html %>%
    html_nodes("a") %>%
    html_attr("href")
  
  # Фильтруем только файлы .parquet
  parquet_files <- links[grepl("\\.parquet$", links)]
  
  if (length(parquet_files) == 0) {
    message("Файлы .parquet не найдены.")
    return()
  }
  
  # Скачиваем каждый файл
  for (file_link in parquet_files) {
    file_name <- basename(file_link) 
    file_url <- paste0(raw_base_url, file_name) # Формируем URL для скачивания
    save_path <- file.path(save_directory, file_name) 
    
    # Скачиваем файл
    download_file(file_url, save_path)
  }
  
  message("Все файлы успешно скачаны!")
}

# Запуск программы
download_parquet_files_from_github()
```


## Running Code

```{r}
library(dplyr)
library(lubridate)

# Загрузка данных
data <- read.csv("combined_datasets.csv", stringsAsFactors = FALSE)

# Преобразование StartSession в формат даты/времени
data$StartSession <- ymd_hms(data$StartSession)

# Агрегация данных по IdSubscriber и StartSession
aggregated_data <- data %>%
  group_by(IdSubscriber, StartSession) %>%
  summarise(
    total_up_traffic = sum(UpTx, na.rm = TRUE),
    total_down_traffic = sum(DownTx, na.rm = TRUE),
    session_count = n()
  ) %>%
  ungroup()

# Вычисление статистик для анализа аномалий
traffic_stats <- aggregated_data %>%
  summarise(
    median_up = median(total_up_traffic, na.rm = TRUE),
    iqr_up = IQR(total_up_traffic, na.rm = TRUE),
    median_down = median(total_down_traffic, na.rm = TRUE),
    iqr_down = IQR(total_down_traffic, na.rm = TRUE)
  )

# Установка границ для аномалий
lower_bound_up <- traffic_stats$median_up - 1.5 * traffic_stats$iqr_up
upper_bound_up <- traffic_stats$median_up + 1.5 * traffic_stats$iqr_up

lower_bound_down <- traffic_stats$median_down - 1.5 * traffic_stats$iqr_down
upper_bound_down <- traffic_stats$median_down + 1.5 * traffic_stats$iqr_down

# Функция для выявления аномалий
aggregated_data <- aggregated_data %>%
  mutate(
    is_anomalous_up = total_up_traffic < lower_bound_up | total_up_traffic > upper_bound_up,
    is_anomalous_down = total_down_traffic < lower_bound_down | total_down_traffic > upper_bound_down,
    is_anomalous = is_anomalous_up | is_anomalous_down
  )

# Фильтрация предположительно взломанных пользователей
anomalous_users <- aggregated_data %>%
  filter(is_anomalous) %>%
  select(IdSubscriber, StartSession, total_up_traffic, total_down_traffic, is_anomalous_up, is_anomalous_down)

# Сохранение списка аномалий в файл
write.csv(anomalous_users, "anomalous_users.csv", row.names = FALSE)

# Просмотр результатов
print("Предположительно взломанные пользователи:")
print(anomalous_users)
```


#перевод .parquet файла в csv. physical.parquet-> .csv
```{r}
library(arrow)
library(base64enc)

# Читаем файл Parquet
file <- "parquet_files/physical.parquet" 
data <- read_parquet(file)

# Обрабатываем столбец Phones (arrow_binary -> Base64)
if ("Phones" %in% colnames(data)) {
  data$Phones <- sapply(data$Phones, function(x) {
    if (!is.null(x)) {
      # Преобразуем бинарные данные в Base64
      base64encode(as.raw(x))
    } else {
      NA  # Если значение NULL, оставляем NA
    }
  })
}

# Записываем данные в CSV
write.csv(data, file = "physical.csv", row.names = FALSE, fileEncoding = "UTF-8")

cat("Файл успешно сохранен как physical.csv\n")

```


#перевод .parquet файла в csv. company.parquet-> .csv
```{r}
library(arrow)
library(base64enc)

# Читаем файл Parquet
file <- "parquet_files/company.parquet" 
data <- read_parquet(file)

# Функция для преобразования arrow_binary в Base64
convert_binary_to_base64 <- function(column) {
  sapply(column, function(x) {
    if (!is.null(x)) {
      base64encode(as.raw(x))  # Преобразуем бинарные данные в Base64
    } else {
      NA  # Если значение NULL, оставляем NA
    }
  })
}

# Преобразуем столбцы с типом arrow_binary (Phones и Contact)
if ("Phones" %in% colnames(data)) {
  data$Phones <- convert_binary_to_base64(data$Phones)
}

if ("Contact" %in% colnames(data)) {
  data$Contact <- convert_binary_to_base64(data$Contact)
}

# Записываем данные в CSV
write.csv(data, file = "company.csv", row.names = FALSE, fileEncoding = "UTF-8")

cat("Файл успешно сохранен как company.csv\n")

```


#перевод .parquet файла в csv. client.parquet-> .csv
```{r}
library(arrow)

# Читаем файл Parquet
file <- "parquet_files/client.parquet"  
data <- read_parquet(file)

# Записываем данные в CSV
write.csv(data, file = "client.csv", row.names = FALSE, fileEncoding = "UTF-8")

cat("Файл успешно сохранен как client.csv\n")

```


#Разделение датасетов на часы
```{r}
# Загрузка необходимых библиотек
library(dplyr)
library(lubridate)

# Загрузка датасета
data <- read.csv("31/dataset_2023-12-31.csv", stringsAsFactors = FALSE)

# Преобразование столбца StartSession в формат POSIXct
data$StartSession <- ymd_hms(data$StartSession, tz = "UTC")

# Добавление нового столбца с часами
data$Hour <- hour(data$StartSession)

# Разделение датасета по часам
split_data <- split(data, data$Hour)

# Сохранение каждого разделённого датасета в отдельный CSV-файл
for (hour in names(split_data)) {
  write.csv(split_data[[hour]], paste0("psx_2024_12_31_", hour, ".csv"), row.names = FALSE)
}

# Сообщение о завершении
cat("Датасет успешно разделён по часам и сохранён в файлы.\n")

```

# Определение аномальных пользователей за каждый час.
```{r}
library(dplyr)
library(lubridate)
input_folder <- "datasets_hour"
output_folder <- "proccessed_datasets"

# Создание папки для сохранения данных
if (!dir.exists(output_folder)) {
  dir.create(output_folder)
}

# Функция для определения аномальности
is_anomalous <- function(value, mean, sd) {
  #Если отклонение больше стандартного в 3 раза, то, вероятно, что трафик аномальный
  abs(value - mean) > 3 * sd
}

# Получение списка всех файлов в папке
files <- list.files(input_folder, pattern = "\\.csv$", full.names = TRUE)

# Считывание данных в каждом фалей
for (file in files) {
  data <- read.csv(file)
  
  
  # Рассчет среднего значения и стандартного отклонения для UpTx и DownTx
  up_mean <- mean(data$UpTx, na.rm = TRUE)
  up_sd <- sd(data$UpTx, na.rm = TRUE)
  down_mean <- mean(data$DownTx, na.rm = TRUE)
  down_sd <- sd(data$DownTx, na.rm = TRUE)
  
  # Определение аномалий
  data$is_hacked <- with(data, 
                         is_anomalous(UpTx, up_mean, up_sd) | 
                         is_anomalous(DownTx, down_mean, down_sd))
  
  # Формирование витрины
  result <- data[, c("IdSubscriber", "date", "Hour", "UpTx", "DownTx", "is_hacked")]
  
  # Сохранение
  output_file <- file.path(output_folder, basename(file))
  write.csv(result, output_file, row.names = FALSE)
  cat("Обработан файл:", basename(file), "\n")
}
cat("Обработка завершена.\n")
```



